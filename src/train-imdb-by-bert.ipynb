{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31caf667-eb74-4787-9a4b-3006dbbc1f3c",
   "metadata": {},
   "source": [
    "# classification with imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e02426-4192-45d1-b7fa-6a49a357f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2caec27-ff95-4073-b428-7e25850c6a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f417cbbe-d628-479e-a646-09102ee5bea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='sentiment'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['sentiment']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be617b-6e80-4c6c-8fdd-a407ba38a794",
   "metadata": {},
   "source": [
    "# prepare for Dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b8a8c0-ca04-49d9-8f91-1f458a043b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef39f0be-fb5b-46a1-8a17-b9217210c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    '''IMDbの前処理'''\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    return text\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {'positive':0,\n",
    "          'negative':1}\n",
    "\n",
    "class BbcDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['sentiment']]\n",
    "        self.texts = [tokenizer(preprocessing_text(text), \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['review']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec70f1c-2baf-44ed-bbd2-f4aea0f73e47",
   "metadata": {},
   "source": [
    "# added Classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9539ae-e60b-4c3e-b5f0-9e5ba5cd5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b1596-d81c-4041-a14c-95e358f5b233",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2bdf67-ebd4-4405-9487-584649beb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = BbcDataset(train_data), BbcDataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611173dc-1e21-40c4-b5d8-28c8dea1dafb",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211b3db7-20ec-4d78-902a-af1bbd8276e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = BbcDataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=32)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            output = model(input_id, mask)\n",
    "                    \n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            y_pred.extend(output.argmax(dim=1).cpu().data.numpy())\n",
    "            y_true.extend(test_label.cpu().data.numpy())\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c177c-b9c6-4e1e-aeb8-b0f90d35c76b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fa813a-ae7c-4d02-acbd-661baa800121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b05b58-18b1-400f-aa87-6445e198d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0% 0/1250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.75 GiB total capacity; 5.00 GiB already allocated; 44.81 MiB free; 5.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BertClassifier()\n\u001b[1;32m      3\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m mask \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m input_id \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(output, train_label\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     33\u001b[0m total_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_id, mask):\n\u001b[0;32m---> 14\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     dropout_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m     16\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dropout_output)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    462\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    463\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 464\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch-py3_8/lib/python3.8/site-packages/torch/nn/functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2484\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[0;32m-> 2486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.75 GiB total capacity; 5.00 GiB already allocated; 44.81 MiB free; 5.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "EPOCHS = 150\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a9a19-7658-4924-a7a6-8ec1c6e3b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a13f1-8df1-46a0-8cf6-0c6c77cfb1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
